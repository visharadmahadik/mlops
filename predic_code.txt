import mlflow.xgboost
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import xgboost as xgb
import pandas as pd
import os
import mlflow
from mlflow import MlflowClient

# Step 1: Load data
data = load_iris()
X = pd.DataFrame(data.data, columns=data.feature_names)
y = data.target

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 4: Load model from MLflow
# run_id = mlflow.active_run().info.run_id
model_uri = f"runs:/848e760bb169434ca048fed75830432f/xgboost_model"
loaded_model = mlflow.xgboost.load_model(model_uri)

# Step 5: Predict on multiple samples one by one
samples = X_test.iloc[:5]  # Select first 5 samples from the test set
print(samples)

print("Predictions:")
for i, sample in samples.iterrows():
    sample = sample.values.reshape(1, -1)
    prediction = loaded_model.predict(sample)
    print(f"Sample {i}: Predicted class = {prediction[0]}")

# Evaluate model accuracy
y_pred = loaded_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Model accuracy on test set: {accuracy}")


<---------------------------------><---------------------------------><---------------------------------><--------------------------------->

import logging
import numpy as np
import json
from sagemaker import get_execution_role
from sagemaker.predictor import Predictor
from sagemaker.session import Session
from sagemaker.base_serializers import NumpySerializer 

# Set logging level to debug to capture detailed logs
logging.basicConfig(level=logging.DEBUG)
endpoint = 'model-name-911100d6d34411efb5fd92ae67fb-2025-01-15-13-28-15-650'

# Example: Using an existing endpoint and setting up a predictor
predictor = Predictor(endpoint_name=endpoint, sagemaker_session=Session())

# Your data (example input data as a NumPy array)
sklearn_input = np.array([4.0, 8.0, 6.0, 3.0]).reshape(1, -1)

# Convert NumPy array to list (as JSON doesn't support NumPy arrays directly)
sklearn_input_list = sklearn_input.tolist()

# Set the serializer to JSON
predictor.serializer = NumpySerializer()

# Triggering prediction, logging will capture the request details
response = predictor.predict(sklearn_input_list)

# Log the response (for debugging and verification)
logging.debug("Prediction response: %s", response)

response


https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_processing.html
Script Location: The script (src/preprocessing.py) is not in the container,
                 but SageMaker automatically ensures it's available inside the container at runtime.
Script Transfer: The code='src/preprocessing.py' parameter tells SageMaker where the script is located. 
                 When the processing job starts, SageMaker copies it to the container.
Environment Execution: Once inside the container, 
                       the script is executed as specified in the command (e.g., python3 src/preprocessing.py),
                       and the data is processed.